{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b49d84-5061-4e13-a920-e2c9684f0553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "TAVILY_API_KEY:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"GOOGLE_API_KEY\")\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "850938e5-6fd8-4d9d-a354-b64010ad2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c387b8d-bbc8-46aa-beef-1207849b9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the prompt is\n",
    "- What variables will be passed into the prompt template\n",
    "- Any constraints for what the output should NOT do\n",
    "- Any requirements that the output MUST adhere to\n",
    "\n",
    "If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discern all the information, call the relevant tool.\"\"\"\n",
    "\n",
    "\n",
    "def get_messages_info(messages):\n",
    "    return [SystemMessage(content=template)] + messages\n",
    "\n",
    "\n",
    "class PromptInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "\n",
    "    objective: str\n",
    "    variables: List[str]\n",
    "    constraints: List[str]\n",
    "    requirements: List[str]\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n",
    "llm_with_tool = llm.bind_tools([PromptInstructions])\n",
    "\n",
    "\n",
    "def info_chain(state):\n",
    "    messages = get_messages_info(state[\"messages\"])\n",
    "    response = llm_with_tool.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21cbbef6-a363-428d-9f65-67196d4dfc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Prompt \n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "# New system prompt\n",
    "prompt_system = \"\"\"Based on the following requirements, write a good prompt template:\n",
    "\n",
    "{reqs}\"\"\"\n",
    "\n",
    "\n",
    "# Function to get the messages for the prompt\n",
    "# Will only get messages AFTER the tool call\n",
    "def get_prompt_messages(messages: list):\n",
    "    tool_call = None\n",
    "    other_msgs = []\n",
    "    for m in messages:\n",
    "        if isinstance(m, AIMessage) and m.tool_calls:\n",
    "            tool_call = m.tool_calls[0][\"args\"]\n",
    "        elif isinstance(m, ToolMessage):\n",
    "            continue\n",
    "        elif tool_call is not None:\n",
    "            other_msgs.append(m)\n",
    "    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs\n",
    "\n",
    "\n",
    "def prompt_gen_chain(state):\n",
    "    messages = get_prompt_messages(state[\"messages\"])\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee4b0973-f2d4-4673-ba04-ab4f764e25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state logic \n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "from langgraph.graph import END\n",
    "\n",
    "\n",
    "def get_state(state):\n",
    "    messages = state[\"messages\"]\n",
    "    if isinstance(messages[-1], AIMessage) and messages[-1].tool_calls:\n",
    "        return \"add_tool_message\"\n",
    "    elif not isinstance(messages[-1], HumanMessage):\n",
    "        return END\n",
    "    return \"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1e19de6-6249-4f7f-b0a4-6bf72baa49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph \n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "memory = MemorySaver()\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"info\", info_chain)\n",
    "workflow.add_node(\"prompt\", prompt_gen_chain)\n",
    "\n",
    "\n",
    "@workflow.add_node\n",
    "def add_tool_message(state: State):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=\"Prompt generated!\",\n",
    "                tool_call_id=state[\"messages\"][-1].tool_calls[0][\"id\"],\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\"info\", get_state, [\"add_tool_message\", \"info\", END])\n",
    "workflow.add_edge(\"add_tool_message\", \"prompt\")\n",
    "workflow.add_edge(\"prompt\", END)\n",
    "workflow.add_edge(START, \"info\")\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbc5f572-5fde-4b39-8265-37e6884b6966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "256971ae-f5a0-48ab-a945-e40d917b4009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  Hi, want to create a prompt \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit): Hi, want to create a prompt \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Great! To create the perfect prompt template, I need a little more information.  Tell me about:\n",
      "\n",
      "1.  **The objective of the prompt:** What are we trying to achieve with this prompt? What should the LLM ultimately *do*?\n",
      "2.  **Variables:** What information will be fed into the prompt as variables? (e.g., product name, customer review, article title, etc.)\n",
      "3.  **Constraints:** Are there any things the LLM should *definitely NOT do* in its response?\n",
      "4.  **Requirements:** Are there any specific requirements the output *must* adhere to? (e.g., be less than 280 characters, include a call to action, etc.)\n",
      "\n",
      "Once I have these details, I can create the perfect prompt instructions!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  creating ai insights from given text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit): creating ai insights from given text\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, that's a good start! To help me create the perfect prompt template, I still need a bit more information.\n",
      "\n",
      "*   **Objective:** Besides creating AI insights, what is the main goal? What should these insights *do* or *achieve*? For example, are they meant to identify key themes, summarize the text, extract specific data points, or something else?\n",
      "*   **Variables:** What kind of text will be fed into the prompt? (e.g., customer reviews, news articles, scientific papers, etc.)\n",
      "*   **Constraints:** Are there any things the LLM should *definitely NOT do* in its response? For example, should it avoid making assumptions, refrain from providing subjective opinions, or stick to a specific format?\n",
      "*   **Requirements:** Are there any specific requirements the output *must* adhere to? (e.g., be less than a certain length, include specific keywords, focus on particular aspects of the text, etc.)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  that ai insights will be used to create a journal what we call it smart journal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit): that ai insights will be used to create a journal what we call it smart journal\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, great! So the AI insights will be used to create a \"smart journal.\" I'm still trying to get a clear picture, so let's clarify a few things:\n",
      "\n",
      "*   **Objective:** What is the primary goal of the AI insights *within the smart journal*? Are they meant to provide summaries, identify key themes, offer reflections, suggest actions, or something else? How will these insights be used by someone reading the journal?\n",
      "*   **Variables:** What kind of text will be fed into the prompt? (e.g., personal reflections, meeting notes, research papers, etc.)\n",
      "*   **Constraints:** What should the AI insights definitely *not* do? (e.g., offer unsolicited advice, make assumptions about the user's feelings, include information outside of the provided text, etc.)\n",
      "*   **Requirements:** What specific requirements must the AI insights meet to be useful for the smart journal? (e.g., be concise, use a specific tone, include actionable steps, etc.)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  voice input converted to text and that text in return is the smart insights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit): voice input converted to text and that text in return is the smart insights\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, that helps a lot! So the input is voice converted to text (so, transcribed speech), and the goal is to generate smart insights from that text for a journal. Let's refine this further:\n",
      "\n",
      "*   **Objective:** What kind of insights should the AI provide based on the transcribed speech? For example, should it identify key topics discussed, summarize action items, reflect on the user's feelings or intentions, or something else entirely? How will these insights be used *in the journal*?\n",
      "*   **Constraints:** What should the AI *definitely NOT do* when generating insights? For example, should it avoid giving advice, making assumptions, or including information not present in the transcribed text? Should it avoid being overly negative or positive?\n",
      "*   **Requirements:** What are the specific requirements for the AI insights? For example, should they be brief and to-the-point? Should they be formatted in a specific way (e.g., bullet points, short paragraphs)? Should they include a call to action or reflection question?\n",
      "\n",
      "The more specific you are, the better I can tailor the prompt instructions!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  mostly the core points of the text, important notes form it etc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit): mostly the core points of the text, important notes form it etc\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, so the objective is to extract core points and important notes from the transcribed speech.\n",
      "\n",
      "*   **Objective:** To extract and summarize the core points and important notes from the transcribed speech, for use in a smart journal.\n",
      "\n",
      "*   **Variables:** The transcribed speech text.\n",
      "\n",
      "*   **Constraints:**\n",
      "    *   The AI should not include information that is not present in the transcribed text.\n",
      "    *   The AI should avoid making assumptions or offering personal opinions.\n",
      "    *   The AI should not provide advice.\n",
      "\n",
      "*   **Requirements:**\n",
      "    *   The output should be concise and to-the-point.\n",
      "    *   The output should focus on the most important information in the text.\n",
      "\n",
      "Does that accurately summarize what you're looking for? If so, I can go ahead and create the prompt instructions.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  yes looks good\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit): yes looks good\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  PromptInstructions (f4df1171-6583-4bc0-be42-3e6495f8c9a9)\n",
      " Call ID: f4df1171-6583-4bc0-be42-3e6495f8c9a9\n",
      "  Args:\n",
      "    requirements: ['The output should be concise and to-the-point.', 'The output should focus on the most important information in the text.']\n",
      "    constraints: ['The AI should not include information that is not present in the transcribed text.', 'The AI should avoid making assumptions or offering personal opinions.', 'The AI should not provide advice.']\n",
      "    objective: To extract and summarize the core points and important notes from the transcribed speech, for use in a smart journal.\n",
      "    variables: ['The transcribed speech text.']\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Prompt generated!\n"
     ]
    },
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents: contents is not specified\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgument\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:192\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m generation_method(**kwargs)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Do not retry for these errors.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:835\u001b[39m, in \u001b[36mGenerativeServiceClient.generate_content\u001b[39m\u001b[34m(self, request, model, contents, retry, timeout, metadata)\u001b[39m\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m response = rpc(\n\u001b[32m    836\u001b[39m     request,\n\u001b[32m    837\u001b[39m     retry=retry,\n\u001b[32m    838\u001b[39m     timeout=timeout,\n\u001b[32m    839\u001b[39m     metadata=metadata,\n\u001b[32m    840\u001b[39m )\n\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/google/api_core/gapic_v1/method.py:131\u001b[39m, in \u001b[36m_GapicCallable.__call__\u001b[39m\u001b[34m(self, timeout, retry, compression, *args, **kwargs)\u001b[39m\n\u001b[32m    129\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompression\u001b[39m\u001b[33m\"\u001b[39m] = compression\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:293\u001b[39m, in \u001b[36mRetry.__call__.<locals>.retry_wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    290\u001b[39m sleep_generator = exponential_sleep_generator(\n\u001b[32m    291\u001b[39m     \u001b[38;5;28mself\u001b[39m._initial, \u001b[38;5;28mself\u001b[39m._maximum, multiplier=\u001b[38;5;28mself\u001b[39m._multiplier\n\u001b[32m    292\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retry_target(\n\u001b[32m    294\u001b[39m     target,\n\u001b[32m    295\u001b[39m     \u001b[38;5;28mself\u001b[39m._predicate,\n\u001b[32m    296\u001b[39m     sleep_generator,\n\u001b[32m    297\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m._timeout,\n\u001b[32m    298\u001b[39m     on_error=on_error,\n\u001b[32m    299\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:153\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     _retry_error_helper(\n\u001b[32m    154\u001b[39m         exc,\n\u001b[32m    155\u001b[39m         deadline,\n\u001b[32m    156\u001b[39m         sleep,\n\u001b[32m    157\u001b[39m         error_list,\n\u001b[32m    158\u001b[39m         predicate,\n\u001b[32m    159\u001b[39m         on_error,\n\u001b[32m    160\u001b[39m         exception_factory,\n\u001b[32m    161\u001b[39m         timeout,\n\u001b[32m    162\u001b[39m     )\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/google/api_core/retry/retry_base.py:212\u001b[39m, in \u001b[36m_retry_error_helper\u001b[39m\u001b[34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[39m\n\u001b[32m    207\u001b[39m     final_exc, source_exc = exc_factory_fn(\n\u001b[32m    208\u001b[39m         error_list,\n\u001b[32m    209\u001b[39m         RetryFailureReason.NON_RETRYABLE_ERROR,\n\u001b[32m    210\u001b[39m         original_timeout,\n\u001b[32m    211\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msource_exc\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/google/api_core/retry/retry_unary.py:144\u001b[39m, in \u001b[36mretry_target\u001b[39m\u001b[34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     result = target()\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.isawaitable(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/google/api_core/timeout.py:130\u001b[39m, in \u001b[36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m] = remaining_timeout\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/google/api_core/grpc_helpers.py:78\u001b[39m, in \u001b[36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m grpc.RpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.from_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mInvalidArgument\u001b[39m: 400 * GenerateContentRequest.contents: contents is not specified\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     18\u001b[39m output = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m graph.stream(\n\u001b[32m     20\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(content=user)]}, config=config, stream_mode=\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m ):\n\u001b[32m     22\u001b[39m     last_message = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(output.values()))[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m]\n\u001b[32m     23\u001b[39m     last_message.pretty_print()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/langgraph/pregel/__init__.py:2527\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2525\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2526\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2527\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.tick(\n\u001b[32m   2528\u001b[39m             [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2529\u001b[39m             timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2530\u001b[39m             get_waiter=get_waiter,\n\u001b[32m   2531\u001b[39m             schedule_task=loop.accept_push,\n\u001b[32m   2532\u001b[39m         ):\n\u001b[32m   2533\u001b[39m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2534\u001b[39m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[32m   2535\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mprompt_gen_chain\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprompt_gen_chain\u001b[39m(state):\n\u001b[32m     27\u001b[39m     messages = get_prompt_messages(state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     response = llm.invoke(messages)\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [response]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1199\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   1194\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1195\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1196\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.\u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mcode_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1197\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1199\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().invoke(\u001b[38;5;28minput\u001b[39m, config, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:371\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    361\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    366\u001b[39m     **kwargs: Any,\n\u001b[32m    367\u001b[39m ) -> BaseMessage:\n\u001b[32m    368\u001b[39m     config = ensure_config(config)\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    370\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m         \u001b[38;5;28mself\u001b[39m.generate_prompt(\n\u001b[32m    372\u001b[39m             [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    373\u001b[39m             stop=stop,\n\u001b[32m    374\u001b[39m             callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    375\u001b[39m             tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    376\u001b[39m             metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    377\u001b[39m             run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    378\u001b[39m             run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    379\u001b[39m             **kwargs,\n\u001b[32m    380\u001b[39m         ).generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    381\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:956\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    947\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    949\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    953\u001b[39m     **kwargs: Any,\n\u001b[32m    954\u001b[39m ) -> LLMResult:\n\u001b[32m    955\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m956\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:775\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    772\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    774\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m             \u001b[38;5;28mself\u001b[39m._generate_with_cache(\n\u001b[32m    776\u001b[39m                 m,\n\u001b[32m    777\u001b[39m                 stop=stop,\n\u001b[32m    778\u001b[39m                 run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    779\u001b[39m                 **kwargs,\n\u001b[32m    780\u001b[39m             )\n\u001b[32m    781\u001b[39m         )\n\u001b[32m    782\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    783\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py:1021\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1019\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(\n\u001b[32m   1022\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1023\u001b[39m     )\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:1275\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   1249\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1251\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1262\u001b[39m     **kwargs: Any,\n\u001b[32m   1263\u001b[39m ) -> ChatResult:\n\u001b[32m   1264\u001b[39m     request = \u001b[38;5;28mself\u001b[39m._prepare_request(\n\u001b[32m   1265\u001b[39m         messages,\n\u001b[32m   1266\u001b[39m         stop=stop,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1273\u001b[39m         tool_choice=tool_choice,\n\u001b[32m   1274\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m     response: GenerateContentResponse = _chat_with_retry(\n\u001b[32m   1276\u001b[39m         request=request,\n\u001b[32m   1277\u001b[39m         **kwargs,\n\u001b[32m   1278\u001b[39m         generation_method=\u001b[38;5;28mself\u001b[39m.client.generate_content,\n\u001b[32m   1279\u001b[39m         metadata=\u001b[38;5;28mself\u001b[39m.default_metadata,\n\u001b[32m   1280\u001b[39m     )\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:210\u001b[39m, in \u001b[36m_chat_with_retry\u001b[39m\u001b[34m(generation_method, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    208\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _chat_with_retry(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, *args, **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = action(retry_state)\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: rs.outcome.result())\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = fn(*args, **kwargs)\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/agenticai/lib/python3.13/site-packages/langchain_google_genai/chat_models.py:204\u001b[39m, in \u001b[36m_chat_with_retry.<locals>._chat_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m google.api_core.exceptions.InvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(\n\u001b[32m    205\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid argument provided to Gemini: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents: contents is not specified\n",
      "During task with name 'prompt' and id '3412c6cd-ab9c-4948-fe93-1d850db6bef8'"
     ]
    }
   ],
   "source": [
    "# Use the graph \n",
    "\n",
    "import uuid\n",
    "\n",
    "cached_human_responses = [\"hi!\", \"rag prompt\", \"1 rag, 2 none, 3 no, 4 no\", \"red\", \"q\"]\n",
    "cached_response_index = 0\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "while True:\n",
    "    try:\n",
    "        user = input(\"User (q/Q to quit): \")\n",
    "    except:\n",
    "        user = cached_human_responses[cached_response_index]\n",
    "        cached_response_index += 1\n",
    "    print(f\"User (q/Q to quit): {user}\")\n",
    "    if user in {\"q\", \"Q\"}:\n",
    "        print(\"AI: Byebye\")\n",
    "        break\n",
    "    output = None\n",
    "    for output in graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user)]}, config=config, stream_mode=\"updates\"\n",
    "    ):\n",
    "        last_message = next(iter(output.values()))[\"messages\"][-1]\n",
    "        last_message.pretty_print()\n",
    "\n",
    "    if output and \"prompt\" in output:\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3656e084-5dfb-4c5f-9ea1-216bc5ebd833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
